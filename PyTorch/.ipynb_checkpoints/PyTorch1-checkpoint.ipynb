{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba8a7875-f4f4-4ef2-b688-fedd302730e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15954b9-f3ea-4539-937f-6174716fa310",
   "metadata": {},
   "source": [
    "pytorch is a python package which enables users to train state-of-the-art machine learning/deep learning models. In order to efficiently use pytorch, one needs to have a firm understanding of the basic building blocks of pytorch: the *torch.tensor* object. In many ways, it's similar to a *numpy array*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d2866a-d6b9-4364-b438-eb70eca39ad2",
   "metadata": {},
   "source": [
    "# Numpy vs. Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a874c-e67c-4008-b638-332a96d71840",
   "metadata": {},
   "source": [
    "Numpy arrays and PyTorch tensors can be created in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2132cf1f-53da-455c-89d0-21b498f28a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9af1aa5b-2275-4246-93a4-9fb47381d0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe02b8bd-c6b3-4ac3-a461-427a4d00348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.linspace(0, 1, 5)\n",
    "t = torch.linspace(0, 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e85baae7-c09e-4e6d-b8d4-3e2ef4130c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.25, 0.5 , 0.75, 1.  ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b3998ec-ac9f-4ca0-8249-3bafa881ad43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaefe03e-c96d-47af-83aa-c6a4562edbf7",
   "metadata": {},
   "source": [
    "They can be resize in similar way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8abcd04-5612-43b9-b789-a61a860f3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.arange(48).reshape(3,4,4) # 3*4*4=48\n",
    "t = torch.arange(48).reshape(3,4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "899f8d5b-8080-46ba-9d91-cbb89043ec27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15]],\n",
       "\n",
       "       [[16, 17, 18, 19],\n",
       "        [20, 21, 22, 23],\n",
       "        [24, 25, 26, 27],\n",
       "        [28, 29, 30, 31]],\n",
       "\n",
       "       [[32, 33, 34, 35],\n",
       "        [36, 37, 38, 39],\n",
       "        [40, 41, 42, 43],\n",
       "        [44, 45, 46, 47]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d343165-af87-40ba-a6c7-4863d47d8745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15]],\n",
       "\n",
       "        [[16, 17, 18, 19],\n",
       "         [20, 21, 22, 23],\n",
       "         [24, 25, 26, 27],\n",
       "         [28, 29, 30, 31]],\n",
       "\n",
       "        [[32, 33, 34, 35],\n",
       "         [36, 37, 38, 39],\n",
       "         [40, 41, 42, 43],\n",
       "         [44, 45, 46, 47]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db159e44-4ac0-458a-ab0b-bbb894117ac3",
   "metadata": {},
   "source": [
    "Most importantly, they have the same broadcasting rules. In order to use pytorch (and even numpy for that matter) most efficiently, one needs to have a strong grasp on the broadcasting rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61b729c-f88c-4553-b6bc-1d41cb53eaff",
   "metadata": {},
   "source": [
    "# General Broadcasting Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18df11d1-f84c-411c-9c49-01ddcc7393c2",
   "metadata": {},
   "source": [
    "When operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing (i.e. rightmost) dimensions and works its way left. Two dimensions are compatible when: <br>\n",
    "1. they are equal, or\n",
    "2. one of them (one of the corresponding shape) is 1 <br>\n",
    "\n",
    "**Example** : The following are compatible <br>\n",
    "Shape 1: (1,6,4,1,7,2) # 6D array<br>\n",
    "Shape 2: (5,6,1,3,1,2) # 6D array <br>\n",
    "\n",
    "because 2=2, among 7 and 1, one is 1, among 1 and 3, one of them is 1 and so on....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cad01c0-ad66-434a-88bb-72dce9aff5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2])\n",
    "b = np.array([3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a379b083-7820-4b99-b6d3-9aad1730a394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "272fd867-4ded-4445-a683-dff817ad028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones((6, 5))\n",
    "b = np.arange(5).reshape((1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14f14c56-ae7e-490e-8548-40bdd26686a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "860a977d-f285-44b2-9c51-d57039894d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7667093a-0008-4579-b2ac-096da1908811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25fc866e-7ad3-4344-9f24-d92e167c9a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4931a0-a7fb-45f8-81ce-6220dd36617a",
   "metadata": {},
   "source": [
    "here, 5=5 and among 6 and 1, one of them is 1. so a and b are compatible for multiplication or addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "769d0b42-c64a-4633-8397-30647b0da2f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b6e764-0b0e-4043-955a-9aa0349cdecb",
   "metadata": {},
   "source": [
    "b is duplicated as the number of rows in a and added or multiplied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d23cf5c5-48f0-40a6-a6b4-0295026dc166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2., 3., 4.],\n",
       "       [0., 1., 2., 3., 4.],\n",
       "       [0., 1., 2., 3., 4.],\n",
       "       [0., 1., 2., 3., 4.],\n",
       "       [0., 1., 2., 3., 4.],\n",
       "       [0., 1., 2., 3., 4.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9faa4f17-21c5-43a8-ae7d-e7117193af44",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((6, 5))\n",
    "b = torch.arange(5).reshape((1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3933de5-b970-44e6-8776-d3de8c95ccc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 5])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14a087fa-b9ba-4311-9882-15991606993b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec9160ef-314e-4af0-9551-da2b7b3c3130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b031c7de-de67-4978-a591-4ff5e1b3d4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3., 4.],\n",
       "        [0., 1., 2., 3., 4.],\n",
       "        [0., 1., 2., 3., 4.],\n",
       "        [0., 1., 2., 3., 4.],\n",
       "        [0., 1., 2., 3., 4.],\n",
       "        [0., 1., 2., 3., 4.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a6235d-3217-4b4b-b34d-69397dc6093d",
   "metadata": {},
   "source": [
    " The arrays/ tensors don't need to have the same number of dimensions. If one of the arrays/ tensors has less dimension than the other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae79eae-8aee-48c7-bf88-60d9145c020a",
   "metadata": {},
   "source": [
    "**Example**: Scaling each other the color channels of an image by different amount: <br>\n",
    "Image (3d array): 256 x 256 x 3 <br>\n",
    "Scale (1d array): &nbsp;    &emsp;  &emsp;  &emsp;  &emsp; 3 <br>\n",
    "Result (3d array): 256 x 256 x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "019614b0-299c-40cd-8d1b-4dc3e20293f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image = torch.randn((256,256,3))\n",
    "Scale = torch.tensor([0.5, 1.5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d61f96e-65fd-4fc5-8f6b-0622c761023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Result = Image * Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84a7fed0-0c59-4788-9881-b91ff9affd69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.1454e-02,  2.6802e-01,  4.1596e-01],\n",
       "         [ 8.4332e-01, -1.8419e-01, -1.4681e-01],\n",
       "         [ 1.1405e-01, -1.6455e+00,  1.4219e-01],\n",
       "         ...,\n",
       "         [-2.6965e-03, -1.4741e+00, -1.2482e+00],\n",
       "         [ 1.1183e+00,  3.6781e-02, -2.3700e-02],\n",
       "         [-1.0798e-01,  2.3609e+00,  1.7246e+00]],\n",
       "\n",
       "        [[-2.3946e-01, -1.4655e+00,  1.4798e+00],\n",
       "         [ 1.0470e+00,  3.8732e-01,  9.2530e-01],\n",
       "         [ 1.4290e-01,  1.6302e-01, -3.4421e-01],\n",
       "         ...,\n",
       "         [-2.2645e-01,  1.9465e+00, -1.5431e+00],\n",
       "         [ 2.0174e-01,  6.5452e-01, -7.3650e-01],\n",
       "         [ 8.1948e-02, -3.9358e-01,  1.2933e+00]],\n",
       "\n",
       "        [[ 2.1527e-01,  5.7599e-01, -1.1575e-01],\n",
       "         [ 9.4873e-02, -1.5287e+00,  7.6560e-02],\n",
       "         [-5.4928e-01, -5.1547e-01, -8.7000e-01],\n",
       "         ...,\n",
       "         [ 3.3562e-01,  8.3634e-01, -1.8612e+00],\n",
       "         [-2.4708e-01,  4.2533e-01,  4.5570e-01],\n",
       "         [ 2.8943e-01,  6.7803e-01, -5.6016e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-6.2291e-01,  3.0958e-01, -9.6128e-01],\n",
       "         [ 3.2480e-01, -7.8823e-01, -7.8160e-01],\n",
       "         [ 3.4296e-01,  1.2065e+00, -1.1418e-01],\n",
       "         ...,\n",
       "         [-1.0282e+00, -3.3752e+00,  1.4664e+00],\n",
       "         [ 1.0081e+00,  6.1026e-02, -4.4371e-01],\n",
       "         [ 6.2543e-01, -1.4429e-02,  5.7134e-01]],\n",
       "\n",
       "        [[ 1.0263e+00, -1.9348e+00, -1.4662e+00],\n",
       "         [ 4.6787e-01, -1.0155e+00, -2.7045e-01],\n",
       "         [ 2.3513e-01,  1.7927e+00, -1.5178e+00],\n",
       "         ...,\n",
       "         [ 9.1422e-01, -2.0619e+00,  6.4665e-01],\n",
       "         [ 6.5599e-01,  1.9743e+00,  4.5034e-01],\n",
       "         [-2.3981e-01,  3.2603e+00, -6.0855e-01]],\n",
       "\n",
       "        [[-6.2366e-01,  2.1726e-01, -5.2684e-01],\n",
       "         [ 1.6099e-01,  2.2641e+00, -9.4527e-01],\n",
       "         [-9.1749e-01, -1.9130e+00, -7.8972e-01],\n",
       "         ...,\n",
       "         [-6.1377e-01,  2.7553e-01,  1.4746e+00],\n",
       "         [ 1.2867e-01, -2.2643e+00,  3.8077e-01],\n",
       "         [-2.5120e-01,  3.6640e+00,  1.0964e+00]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b41d2f-6a24-41e3-b0f3-a9d7fd5e5394",
   "metadata": {},
   "source": [
    "**Example**: One has an array of 2 images and wants to scale the color channels of each image by a slightly different amount: <br>\n",
    "Images (4d array): 2 x 256 x 256 x 3 <br>\n",
    "Scales (4d array): 2 x 1 x 1 x 3 <br>\n",
    "Results (4d array): 2 x 256 x 256 x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c7903843-f96f-49f7-b270-89d8e4bdac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "Images = torch.randn((2, 256, 256, 3))\n",
    "Scales =  torch.tensor([0.5, 1.5, 1, 1.5, 1, 0.5]).reshape((2,1,1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a52663db-4425-4feb-a5b7-ddb6c08b40fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.5000, 1.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[1.5000, 1.0000, 0.5000]]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e581bc8-e369-4c5d-bb63-265866bfb1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = Images * Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f1b72c8-778e-4119-8f66-19facb60315f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 6.9224e-01,  2.1387e+00,  5.6479e-01],\n",
       "          [ 1.0987e+00,  2.5955e+00, -1.7519e+00],\n",
       "          [-4.2801e-02, -2.4423e+00,  6.6390e-01],\n",
       "          ...,\n",
       "          [ 1.1118e+00, -1.6474e+00,  5.6583e-01],\n",
       "          [ 8.6046e-01, -9.1463e-01, -7.8563e-02],\n",
       "          [-5.9368e-02,  1.0753e+00,  2.1903e-02]],\n",
       "\n",
       "         [[-4.9166e-01,  3.0684e+00, -9.7192e-02],\n",
       "          [-4.2931e-01,  1.1056e+00, -8.6740e-01],\n",
       "          [-4.0219e-01, -1.1884e+00,  6.3587e-01],\n",
       "          ...,\n",
       "          [ 3.4750e-01, -4.9126e-01,  2.7277e+00],\n",
       "          [ 2.8576e-01,  1.1128e+00,  2.2119e-01],\n",
       "          [-1.7568e-01,  2.4408e-01,  6.9873e-01]],\n",
       "\n",
       "         [[-2.8574e-01,  1.3898e+00, -1.5342e+00],\n",
       "          [-1.2999e-01, -1.1167e+00, -6.7655e-01],\n",
       "          [-3.4302e-01,  5.1024e-01,  6.4373e-01],\n",
       "          ...,\n",
       "          [-5.2884e-01,  2.3116e-01, -4.7809e-01],\n",
       "          [ 6.1831e-01,  3.3039e-01, -5.3643e-01],\n",
       "          [ 3.2004e-01, -2.3021e+00,  4.7926e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.4149e-01, -1.3962e+00, -5.5957e-01],\n",
       "          [ 4.0938e-01, -1.5127e-01, -1.1232e+00],\n",
       "          [-4.8864e-01,  4.6208e-01,  1.0085e+00],\n",
       "          ...,\n",
       "          [ 4.8111e-01, -2.3911e+00,  3.5081e-01],\n",
       "          [-1.0824e-01, -2.6397e+00,  9.5609e-03],\n",
       "          [ 2.2457e-01, -2.1134e-01, -1.9536e-01]],\n",
       "\n",
       "         [[-5.7441e-01, -1.6053e+00, -1.0938e+00],\n",
       "          [ 8.8903e-02, -1.0021e+00,  1.3516e+00],\n",
       "          [ 5.8570e-01,  1.3314e+00, -2.6084e-01],\n",
       "          ...,\n",
       "          [ 1.6081e-01, -1.0126e-01, -1.0719e+00],\n",
       "          [-3.1242e-01,  2.6862e+00, -5.8285e-01],\n",
       "          [ 1.0731e-01, -2.8830e+00,  5.0335e-01]],\n",
       "\n",
       "         [[ 3.1584e-02, -1.4355e+00, -5.0107e-01],\n",
       "          [-9.2266e-02, -1.9583e-01,  9.2812e-01],\n",
       "          [ 1.9370e-01, -3.7726e-01,  1.4872e-01],\n",
       "          ...,\n",
       "          [ 8.9207e-01,  2.4609e-01, -2.6250e-01],\n",
       "          [-3.4584e-01,  3.5085e-01, -6.3057e-01],\n",
       "          [ 4.5005e-01, -5.5318e-01,  4.2407e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.4875e+00,  6.3229e-01, -6.6858e-01],\n",
       "          [-5.3175e-01,  1.3871e-01, -2.7529e-01],\n",
       "          [-2.4579e+00, -1.3995e-01, -4.5093e-01],\n",
       "          ...,\n",
       "          [-2.5185e+00,  2.8424e-01, -8.2428e-02],\n",
       "          [ 2.0152e+00, -6.2187e-01, -2.4006e-01],\n",
       "          [-8.5982e-01,  9.1628e-01,  2.3192e-01]],\n",
       "\n",
       "         [[ 1.6495e-01, -7.4266e-01, -2.0606e-01],\n",
       "          [-1.7089e-01,  1.0019e+00, -3.7386e-01],\n",
       "          [ 7.3917e-02, -4.3686e-01,  3.3306e-01],\n",
       "          ...,\n",
       "          [ 6.6652e-01, -7.0822e-01, -7.4748e-01],\n",
       "          [-6.5493e-01,  3.7511e-01,  6.6931e-01],\n",
       "          [ 6.5463e-01, -3.0863e-01, -1.0458e-01]],\n",
       "\n",
       "         [[ 2.8717e+00,  5.6454e-01, -8.3993e-01],\n",
       "          [ 8.2515e-01,  1.3528e+00,  4.9418e-01],\n",
       "          [-5.0181e-01, -7.6260e-01,  6.1418e-01],\n",
       "          ...,\n",
       "          [-1.1468e+00,  5.8265e-01, -2.2248e-01],\n",
       "          [-1.3715e+00,  9.9284e-01,  1.3972e-01],\n",
       "          [-1.2251e+00,  1.3545e-01,  3.5517e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.0923e+00,  1.6531e-01,  2.6182e-01],\n",
       "          [ 2.6971e-01,  7.5633e-01, -4.6453e-01],\n",
       "          [ 2.8689e+00, -6.3790e-01, -1.1139e+00],\n",
       "          ...,\n",
       "          [-3.0239e+00, -1.0146e-02, -1.3388e-01],\n",
       "          [ 3.5286e-01,  1.4090e-01,  6.2057e-01],\n",
       "          [ 1.0131e+00,  1.9407e-03,  1.3488e-02]],\n",
       "\n",
       "         [[ 9.0317e-02,  4.5387e-01,  2.5221e-01],\n",
       "          [-1.1612e+00,  7.6598e-01,  4.5209e-01],\n",
       "          [-2.3468e+00,  5.8031e-01,  2.3618e-01],\n",
       "          ...,\n",
       "          [-2.8865e+00,  4.4157e-01, -1.3253e-01],\n",
       "          [ 1.0083e+00, -2.7678e-01,  7.8161e-02],\n",
       "          [-1.3139e+00,  4.5465e-01, -7.7147e-01]],\n",
       "\n",
       "         [[ 2.8335e-01,  1.2074e+00,  5.7431e-01],\n",
       "          [ 1.1834e+00,  1.4775e+00, -1.4993e-01],\n",
       "          [ 2.6346e+00, -1.0390e+00, -3.3391e-01],\n",
       "          ...,\n",
       "          [ 1.9262e+00,  7.1826e-01, -4.4251e-01],\n",
       "          [-1.4948e+00,  1.1959e+00,  6.2746e-01],\n",
       "          [ 4.4948e-01, -1.1737e+00,  3.0295e-01]]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c4a38d-62fd-42f5-80e0-935cadf70fb2",
   "metadata": {},
   "source": [
    "# Operations Across Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c29172a-72a4-47ae-990f-8dd05b8cab1c",
   "metadata": {},
   "source": [
    "This is so fundamental for pytorch. Obviously simple operations can be done on 1 dimensional tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63330e4f-1c91-4b51-8735-0ab43b122e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.1250), tensor(1.6520), tensor(4.), tensor(0.5000))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([0.5, 1, 3, 4])\n",
    "torch.mean(t), torch.std(t), torch.max(t), torch.min(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49189204-5de5-4cf6-9b3f-a9bacadfb750",
   "metadata": {},
   "source": [
    "But suppose we have a 2d tensor, for example, and we want to compute the mean value of each columns: <br>\n",
    "* Note: taking the mean of each column means taking the mean **across** the rows (which are the first dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "61201f8d-5ead-4ecf-8c0a-af8ec2d29333",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(20, dtype=float).reshape(5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe6fe998-ef5e-4b7f-9d94-3a097229737e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c059f1b3-eaa1-405a-9376-e2decaf52372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.,  9., 10., 11.], dtype=torch.float64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117e8a2f-b479-44b3-9f6c-b45d8c6b7b5e",
   "metadata": {},
   "source": [
    "This can be done for higher dimensional arrays as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2470a4f-61b0-4aab-94d5-31f3245e69b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(5, 256, 256, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69153d77-6657-4a46-b91f-0824d4a07ee3",
   "metadata": {},
   "source": [
    "Take the mean across the batch (size 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "61fadb7b-49e2-4063-87f8-4dd002c01694",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5862,  0.2114,  0.8305],\n",
       "         [ 0.1073,  0.1592,  0.1144],\n",
       "         [ 0.2857, -0.0277,  0.5187],\n",
       "         ...,\n",
       "         [-0.1007, -0.4852, -0.3457],\n",
       "         [-0.2784,  0.5910, -0.2158],\n",
       "         [-0.4625, -0.5209, -0.3702]],\n",
       "\n",
       "        [[-0.6171,  0.4625, -0.1960],\n",
       "         [ 1.2811, -0.3595,  0.0391],\n",
       "         [ 0.0184, -0.5875, -0.2994],\n",
       "         ...,\n",
       "         [-0.6224,  0.1476,  0.5807],\n",
       "         [ 0.6610, -0.3446,  0.4007],\n",
       "         [ 0.4598,  0.0181, -0.2860]],\n",
       "\n",
       "        [[-0.9435, -0.2387, -0.1080],\n",
       "         [-0.4153, -0.3182, -0.7225],\n",
       "         [-0.1607,  0.1500, -0.5762],\n",
       "         ...,\n",
       "         [ 0.2325,  0.5990, -0.3596],\n",
       "         [-0.3315, -0.9168, -1.4226],\n",
       "         [-0.4230,  0.4585,  0.3777]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.3885, -0.0937,  0.5384],\n",
       "         [-0.0184, -0.3899,  0.4461],\n",
       "         [-0.2193,  0.1354, -0.0564],\n",
       "         ...,\n",
       "         [ 0.4098, -0.4612,  0.0108],\n",
       "         [ 0.3455, -0.0405, -0.0165],\n",
       "         [ 0.2688,  0.3338, -0.5377]],\n",
       "\n",
       "        [[ 0.1194, -0.1886, -0.3318],\n",
       "         [ 0.9321, -0.0106,  0.7631],\n",
       "         [ 0.4447,  0.6316,  0.4731],\n",
       "         ...,\n",
       "         [-0.3108,  0.1474, -0.2016],\n",
       "         [-0.0167,  0.3238,  0.1499],\n",
       "         [ 0.0311,  0.1846, -0.5962]],\n",
       "\n",
       "        [[-1.1230,  0.1411, -0.6214],\n",
       "         [ 0.3348,  0.7006,  0.0586],\n",
       "         [-0.6211, -0.2794,  0.3788],\n",
       "         ...,\n",
       "         [-0.8313, -0.0532, -0.1868],\n",
       "         [-0.0304,  0.7411,  0.0154],\n",
       "         [-0.2445, -0.1855, -0.4276]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d445f3fb-32a1-42f4-a7e8-a54250807c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 256, 3])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t, axis=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daed0bf-615d-438f-836e-49b261361dbe",
   "metadata": {},
   "source": [
    "Take the mean across the color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "113baa12-1486-4b71-a52b-d01b43bf7663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.8287e-01,  2.5946e-01, -6.8375e-01,  ..., -1.0806e-01,\n",
       "           4.4483e-01, -2.0371e-02],\n",
       "         [-7.5231e-01, -3.9452e-01, -9.9473e-02,  ..., -7.2902e-01,\n",
       "           1.5516e+00,  3.7451e-01],\n",
       "         [-2.2500e-01,  1.4093e-01, -2.6268e-01,  ...,  2.6568e-01,\n",
       "          -9.8937e-01,  8.5870e-01],\n",
       "         ...,\n",
       "         [ 4.6504e-01,  9.4296e-01, -3.6420e-01,  ..., -1.0970e-01,\n",
       "          -1.4246e-02, -4.1372e-01],\n",
       "         [ 4.1170e-01,  1.0829e+00,  3.8034e-01,  ..., -4.1560e-02,\n",
       "           1.4081e+00, -3.2450e-01],\n",
       "         [-2.0361e+00,  1.4021e+00,  5.6690e-01,  ..., -1.2001e+00,\n",
       "           6.1863e-01,  1.1562e-01]],\n",
       "\n",
       "        [[ 1.1500e+00,  9.0068e-02,  1.4117e+00,  ..., -1.2572e-02,\n",
       "           9.7607e-01, -1.0760e+00],\n",
       "         [-4.5693e-01, -1.2365e-01,  1.9171e-01,  ...,  5.7745e-01,\n",
       "          -1.2848e+00, -8.4582e-02],\n",
       "         [ 3.5998e-01, -8.5583e-01, -6.8098e-01,  ..., -1.8599e-01,\n",
       "          -6.1946e-01,  7.0077e-01],\n",
       "         ...,\n",
       "         [ 5.0455e-01,  1.0774e-02, -9.6609e-01,  ...,  6.6381e-01,\n",
       "          -4.7154e-01, -5.6389e-01],\n",
       "         [-1.0294e+00, -2.5015e-01,  3.8930e-01,  ...,  8.7985e-01,\n",
       "          -5.8386e-02,  6.7676e-02],\n",
       "         [ 2.1630e-01,  5.4178e-02, -6.4828e-02,  ..., -4.4573e-01,\n",
       "          -4.5783e-01, -7.6755e-01]],\n",
       "\n",
       "        [[ 3.8154e-01, -2.2709e-01, -4.7545e-01,  ..., -4.8645e-01,\n",
       "          -4.5051e-01,  5.5465e-02],\n",
       "         [ 2.0463e-01,  7.9707e-01, -1.8132e-01,  ...,  3.9709e-01,\n",
       "          -8.2503e-01,  4.2131e-01],\n",
       "         [-8.2345e-01, -5.7632e-01, -8.5364e-01,  ...,  6.3567e-01,\n",
       "          -1.6682e+00,  5.7455e-01],\n",
       "         ...,\n",
       "         [-5.8726e-01, -2.0165e-01,  7.2694e-01,  ..., -7.2330e-01,\n",
       "           1.2538e-01, -6.8368e-01],\n",
       "         [ 2.7903e-01,  3.0019e-01,  6.6801e-01,  ..., -1.0221e+00,\n",
       "          -6.5792e-01, -4.7543e-01],\n",
       "         [-1.1575e-01,  1.7246e-01, -8.0527e-02,  ...,  3.5947e-02,\n",
       "           3.8501e-01, -1.8002e-01]],\n",
       "\n",
       "        [[ 7.7024e-01,  1.4387e+00,  3.1751e-01,  ..., -3.3188e-01,\n",
       "          -1.9187e-01, -2.4380e-01],\n",
       "         [ 3.0167e-01,  1.1248e+00, -6.7137e-01,  ..., -4.3904e-01,\n",
       "           4.5470e-01, -3.0302e-01],\n",
       "         [-1.1655e+00, -9.0755e-01,  7.3659e-01,  ...,  6.1763e-01,\n",
       "          -4.8066e-01, -1.2887e+00],\n",
       "         ...,\n",
       "         [ 2.1209e-01, -7.0198e-01,  8.1813e-01,  ..., -1.4934e-01,\n",
       "           7.6291e-01,  9.3195e-01],\n",
       "         [-3.2286e-01,  5.6607e-01,  3.0353e-01,  ...,  3.8738e-01,\n",
       "           2.6535e-01, -1.6498e-01],\n",
       "         [-1.2619e-01,  3.2460e-01, -3.7908e-01,  ...,  4.5387e-02,\n",
       "           7.5894e-01,  1.7299e-03]],\n",
       "\n",
       "        [[ 2.8857e-02, -9.2623e-01,  7.2432e-01,  ..., -6.1369e-01,\n",
       "          -6.1715e-01, -9.7125e-01],\n",
       "         [ 1.1859e-01,  1.9759e-01, -6.8697e-01,  ...,  3.7004e-01,\n",
       "           1.2988e+00, -8.8414e-02],\n",
       "         [-2.9636e-01, -2.2792e-01,  8.2612e-02,  ..., -5.4649e-01,\n",
       "          -6.9389e-01, -1.5682e-01],\n",
       "         ...,\n",
       "         [ 7.9413e-01,  1.2946e-02, -4.4868e-01,  ...,  2.5082e-01,\n",
       "           7.8356e-02,  8.3762e-01],\n",
       "         [-6.6678e-03,  1.1087e+00,  8.4107e-01,  ..., -8.1212e-01,\n",
       "          -1.9538e-01,  2.6306e-01],\n",
       "         [-6.1030e-01, -1.3000e-01, -9.1204e-01,  ..., -2.2106e-01,\n",
       "          -9.4471e-02, -5.9909e-01]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bb5e52d3-8732-4ceb-9e96-06b404aff97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 256, 256])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t, axis=-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b131416a-2f7c-46db-82aa-8668597a7e27",
   "metadata": {},
   "source": [
    "Take only the maximum color channel values, (and get the corresponding indices): <br>\n",
    "* This is done by all the time in image segmentation models (i.e. take an image, decide which pixels correspond to, say, a car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a989c36f-927e-4bcd-ab4f-8a03141a5dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = torch.max(t, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d096e730-cdd4-4b22-aaa5-502e73da7c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.0340,  1.1700, -0.0560,  ...,  0.3090,  1.5074,  0.7787],\n",
       "          [ 0.3300,  0.6722,  0.7266,  ...,  0.2052,  2.8091,  0.9162],\n",
       "          [ 1.2477,  1.5044,  0.6295,  ...,  0.7073, -0.6769,  1.4490],\n",
       "          ...,\n",
       "          [ 1.2723,  1.4845,  0.2509,  ...,  0.4104,  0.8282,  0.2327],\n",
       "          [ 1.9201,  2.3256,  2.3640,  ...,  0.2043,  2.0907,  0.3436],\n",
       "          [-0.8064,  1.7442,  1.3520,  ..., -0.2068,  1.3393,  0.2322]],\n",
       " \n",
       "         [[ 1.8308,  0.3280,  2.2460,  ...,  0.2707,  1.8919, -0.5982],\n",
       "          [-0.1824,  0.0938,  0.3293,  ...,  0.8796, -0.0886,  0.7682],\n",
       "          [ 2.0298, -0.2823, -0.2881,  ...,  0.7440, -0.2202,  1.5713],\n",
       "          ...,\n",
       "          [ 0.8017,  1.0445, -0.1980,  ...,  1.0038, -0.2186,  0.2398],\n",
       "          [ 0.2863,  1.0484,  1.4741,  ...,  1.2144,  0.1738,  1.1143],\n",
       "          [ 0.8300,  0.7556,  0.4555,  ...,  0.0966, -0.0131,  0.4326]],\n",
       " \n",
       "         [[ 1.1217,  0.9255, -0.1270,  ...,  0.0340,  1.1446,  1.0615],\n",
       "          [ 0.6358,  2.7055,  0.4000,  ...,  1.8068,  0.2508,  1.6299],\n",
       "          [-0.3844,  0.4592, -0.3240,  ...,  0.8795,  0.1503,  1.8228],\n",
       "          ...,\n",
       "          [ 0.0826,  1.3787,  1.4771,  ...,  0.3534,  0.7430,  2.0499],\n",
       "          [ 1.3839,  0.7875,  1.2858,  ..., -0.1564, -0.0453,  0.3876],\n",
       "          [ 0.5868,  1.0469,  1.1334,  ...,  1.8407,  0.7905,  0.7787]],\n",
       " \n",
       "         [[ 1.8734,  2.1964,  2.0087,  ...,  0.6444,  0.8188,  0.3786],\n",
       "          [ 2.7796,  2.1941,  0.1078,  ...,  1.2526,  1.3242,  1.0701],\n",
       "          [-0.3800, -0.7079,  1.4437,  ...,  1.7252, -0.2717, -1.0954],\n",
       "          ...,\n",
       "          [ 0.9825,  0.0457,  1.5936,  ...,  1.1625,  1.4104,  1.9193],\n",
       "          [ 0.1126,  1.8508,  0.6827,  ...,  1.3909,  0.7492,  1.3558],\n",
       "          [ 1.9649,  0.8019,  0.2824,  ...,  1.3315,  1.6988,  0.7737]],\n",
       " \n",
       "         [[ 0.6404,  0.1902,  1.4714,  ...,  0.1173,  0.1342,  0.7365],\n",
       "          [ 0.4316,  0.9281,  0.7647,  ...,  2.2959,  1.8383,  0.5622],\n",
       "          [-0.1287,  1.3035,  0.7206,  ...,  0.3289,  0.0518,  1.1276],\n",
       "          ...,\n",
       "          [ 1.5383,  0.2020,  0.5615,  ...,  0.6080,  0.8497,  2.0336],\n",
       "          [ 0.4997,  3.4299,  1.2033,  ..., -0.6229,  0.4987,  0.7404],\n",
       "          [-0.0323,  0.4316,  0.4602,  ...,  1.0111,  0.5354,  1.3767]]]),\n",
       " tensor([[[0, 2, 0,  ..., 1, 0, 2],\n",
       "          [2, 0, 0,  ..., 2, 0, 0],\n",
       "          [1, 0, 1,  ..., 2, 1, 0],\n",
       "          ...,\n",
       "          [0, 0, 1,  ..., 1, 0, 1],\n",
       "          [0, 0, 1,  ..., 0, 0, 1],\n",
       "          [1, 1, 2,  ..., 2, 1, 0]],\n",
       " \n",
       "         [[1, 0, 2,  ..., 0, 1, 0],\n",
       "          [1, 1, 0,  ..., 1, 2, 2],\n",
       "          [2, 1, 1,  ..., 1, 0, 1],\n",
       "          ...,\n",
       "          [1, 2, 0,  ..., 0, 2, 1],\n",
       "          [2, 1, 0,  ..., 1, 0, 0],\n",
       "          [2, 1, 2,  ..., 2, 2, 0]],\n",
       " \n",
       "         [[2, 0, 1,  ..., 0, 1, 0],\n",
       "          [1, 0, 0,  ..., 2, 0, 1],\n",
       "          [2, 2, 1,  ..., 1, 0, 1],\n",
       "          ...,\n",
       "          [2, 2, 1,  ..., 0, 1, 0],\n",
       "          [1, 2, 1,  ..., 2, 1, 2],\n",
       "          [1, 1, 1,  ..., 2, 1, 2]],\n",
       " \n",
       "         [[2, 1, 2,  ..., 0, 1, 1],\n",
       "          [1, 0, 2,  ..., 1, 0, 0],\n",
       "          [1, 0, 0,  ..., 0, 2, 0],\n",
       "          ...,\n",
       "          [2, 0, 0,  ..., 0, 0, 1],\n",
       "          [1, 2, 0,  ..., 1, 2, 0],\n",
       "          [1, 0, 0,  ..., 1, 1, 1]],\n",
       " \n",
       "         [[2, 2, 1,  ..., 1, 2, 2],\n",
       "          [2, 0, 0,  ..., 2, 1, 1],\n",
       "          [2, 1, 2,  ..., 1, 0, 2],\n",
       "          ...,\n",
       "          [0, 1, 2,  ..., 2, 0, 1],\n",
       "          [0, 0, 2,  ..., 0, 2, 1],\n",
       "          [2, 0, 2,  ..., 1, 0, 1]]]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb816528-7e38-4489-848f-233f6fbf9bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 256, 256])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f45a98-658e-46cb-9713-8442d35103dd",
   "metadata": {},
   "source": [
    "# So where do pytorch and numpy differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc12a042-2327-44e7-8513-362dfef6985a",
   "metadata": {},
   "source": [
    "**PyTorch** starts to really differ from **Numpy** in terms of automatically computing gradients of operations. \n",
    "$$ y = \\sum_{i} x_i^3 $$\n",
    "has a gradient\n",
    "$$ \\frac{\\delta y}{\\delta x_i} = 3x_i^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "67f6a8f9-f287-47e8-8ec3-93e979fc7bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(917., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[5.,8.],[4.,6.]], requires_grad=True)\n",
    "y = x.pow(3).sum()\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd14280-7285-441f-b7a9-24139128fd95",
   "metadata": {},
   "source": [
    "Compute the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4901a0a6-d6c2-470d-8f97-b8f0a72521d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward() # compute the gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "478486b4-fd93-4d8f-9673-3996b2e49a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 75., 192.],\n",
       "        [ 48., 108.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad #print the gradient (everything that has happened to x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d97c15b-22ca-453b-b9ea-7c4b600043c2",
   "metadata": {},
   "source": [
    "Double check using gradient formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "620bba97-3c17-4bec-9ce6-b26b9701ad05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 75., 192.],\n",
       "        [ 48., 108.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c12799-770c-49c0-bc7b-734ec3546171",
   "metadata": {},
   "source": [
    "The automatic computation of gradients is the backbone of training deep learning models. Unlike in the example above, most gradient computations don't have an analytical formula, so the automatic computation of gradients is essential. In general, if one has\n",
    "$$ y = (\\vec{x})$$\n",
    "\n",
    "Then pytorch can compute $\\frac{\\delta y}{\\delta x_i}$\n",
    ". For each of element of the vector \n",
    ". In the context of machine learning, \n",
    " contains all the weights (also known as parameters) of the neural network and \n",
    " is the Loss Function of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08963d47-f3ca-4ff9-91ab-6cd3edcf3849",
   "metadata": {},
   "source": [
    "# Additional Benefits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931590bb-074c-4230-a017-d3bd6bd04fbf",
   "metadata": {},
   "source": [
    "**In addition, any sort of large matrix multiplication problem is faster with torch tensors than it is with numpy arrays, especially if you're running on a GPU.** <br>\n",
    "\n",
    "Using torch: (with a CPU. With GPU, this is much much faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d97c5d9d-b931-411a-b10e-c1e0c86c8686",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn((1000,1000))\n",
    "B = torch.randn((1000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "259bbebe-6248-4200-ade6-988bc9577058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010379899998952169\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "torch.matmul(A,B)\n",
    "t2 = time.perf_counter()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9108eeaa-334a-465f-853c-e04ba732f42b",
   "metadata": {},
   "source": [
    "Using numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2c289768-ac01-43a5-9ba5-7a7f1865484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randn(int(1e6)).reshape((1000,1000))\n",
    "B = np.random.randn(int(1e6)).reshape((1000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cbc3eba3-7e40-43ab-bca8-552a4c93e28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007250399999975343\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "A@B\n",
    "t2 = time.perf_counter()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7bd9ae-45ea-44ed-9cc7-8c11555bd0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forklift",
   "language": "python",
   "name": "forklift"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
